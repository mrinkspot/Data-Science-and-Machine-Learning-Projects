{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# from nltk.corpus import stopwords\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# from nltk.tokenize import word_tokenize\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos y Preprocesamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos el dataset de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review_es</th>\n",
       "      <th>sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>uno de los otros critic ha mencion que despues...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>una pequeñ pequeñ produccion la tecnic de film...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>pens que esta era una maner maravill de pas ti...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>basic hay una famili dond un niñ pequeñ jak pi...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>el amor en el tiemp de pett mattei es una peli...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                          review_es sentimiento\n",
       "0   0  uno de los otros critic ha mencion que despues...    positivo\n",
       "1   1  una pequeñ pequeñ produccion la tecnic de film...    positivo\n",
       "2   2  pens que esta era una maner maravill de pas ti...    positivo\n",
       "3   3  basic hay una famili dond un niñ pequeñ jak pi...    negativo\n",
       "4   4  el amor en el tiemp de pett mattei es una peli...    positivo"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./train_normalizado_stemming.csv\")\n",
    "\n",
    "ds_trabajo = df.copy()\n",
    "ds_trabajo.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función para mostrar las métricas y la matriz de confusión de nuestras predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_metricas(y_test, y_pred):\n",
    "    print(\"La accuracy es {}\".format(accuracy_score(y_test, y_pred)))\n",
    "    print(\"La precision es {}\".format(precision_score(y_test, y_pred, average='macro')))\n",
    "    print(\"El recall es {}\".format(recall_score(y_test, y_pred, average='macro')))\n",
    "    print(\"El F1 es {}\".format(f1_score(y_test, y_pred, average='macro')))\n",
    "\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    sns.heatmap(cm, cmap='Blues',annot=True,fmt='g')\n",
    "    plt.xlabel('Valores Predicción')\n",
    "    plt.ylabel('Valores Reales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos las stopwords en español de la bibliotecta nltk para eliminarlas de los textos posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a aplicar las técnicas de lemmatización y stemming para reducir la cantidad y la complejidad de las palabras en los textos, generando dos datasets distintos con estos preprocesamientos para utilizar a lo largo del trabajo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: vamos a dejar los preprocesamientos aplicados comentados y los guardaremos en los csv `train_normalizado_lematizacion.csv` y `train_normalizado_stemming.csv`. Vamos a hacer lo mismo para `test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       la mayor virtud de esta película es su existen...\n",
       "1       no soy un experto cinéfilo  pero pocas veces m...\n",
       "2       si no eres un incondicional del humor estilo t...\n",
       "3       no sé qué está pasando  si la gente se deja ll...\n",
       "4       pero cuando amanece y me quedo solo siento en ...\n",
       "                              ...                        \n",
       "8594    buena no  lo siguiente  por fin un film serio ...\n",
       "8595    me esperaba mucho  pero que mucho  más guión m...\n",
       "8596    de mal cuerpo como sensación al finalizar  de ...\n",
       "8597    los que han añadido comentarios os lo han dich...\n",
       "8598    fui a ver esta película de cine con entusiasmo...\n",
       "Name: review_es, Length: 8599, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Aplicamos lemmatizacion\n",
    "# import re\n",
    "# ds_trabajo.review_es = ds_trabajo.review_es.str.lower();\n",
    "# ds_trabajo.review_es = ds_trabajo.review_es.apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
    "# ds_trabajo.review_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# # Realizar la lematización en la columna 'Texto'\n",
    "# ds_trabajo.review_es = ds_trabajo.review_es.apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "\n",
    "# print(ds_trabajo.review_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       la mayor virtud de esta pelicul es su existent...\n",
      "1       no soy un expert cinefil per poc vec me hab se...\n",
      "2       si no eres un incondicional del humor estil te...\n",
      "3       no se que esta pas si la gent se dej llev por ...\n",
      "4       per cuand amanec y me qued sol sient en el fon...\n",
      "                              ...                        \n",
      "8594    buen no lo siguient por fin un film seri con u...\n",
      "8595    me esper much per que much mas guion muy vist ...\n",
      "8596    de mal cuerp com sensacion al finaliz de histo...\n",
      "8597    los que han añad comentari os lo han dich clar...\n",
      "8598    fui a ver esta pelicul de cin con entusiasm lo...\n",
      "Name: review_es, Length: 8599, dtype: object\n"
     ]
    }
   ],
   "source": [
    "## Aplicamos stemming\n",
    "# from nltk.stem import SnowballStemmer\n",
    "\n",
    "# stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "# ds_trabajo.review_es = ds_trabajo.review_es.apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "# print(ds_trabajo.review_es)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo 1: Bayes Naïve con CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como primer modelo vamos a probar Bayes Naïve tokenizando con CountVectorizer y eliminando las stopwords. Usaremos un clasificador multinomial y técnica de Laplace Smoothing con alpha = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La accuracy es 0.8335\n",
      "La precision es 0.8359621578099838\n",
      "El recall es 0.8338323103577622\n",
      "El F1 es 0.8332827323896776\n"
     ]
    }
   ],
   "source": [
    "preprocesador = CountVectorizer(stop_words=stopwords)\n",
    "\n",
    "textos_entrenamiento, textos_prueba, etiquetas_entrenamiento, etiquetas_prueba = train_test_split(ds_trabajo.review_es, ds_trabajo.sentimiento, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizador = preprocesador.fit(textos_entrenamiento)\n",
    "caracteristicas_entrenamiento = vectorizador.transform(textos_entrenamiento)\n",
    "caracteristicas_prueba = vectorizador.transform(textos_prueba)\n",
    "\n",
    "modelo = MultinomialNB(alpha=1)\n",
    "modelo.fit(caracteristicas_entrenamiento, etiquetas_entrenamiento)\n",
    "\n",
    "predicciones = modelo.predict(caracteristicas_prueba)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que las métricas dan todas similares, cercanas al 83%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostrar_metricas(etiquetas_prueba, predicciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportamos el modelo en formato pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('./models/naive_bayes/nb_model.pickle', 'wb') as archivo:\n",
    "#     pickle.dump(modelo, archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./models/naive_bayes/nb_model.pickle', 'rb') as archivo:\n",
    "#     modelo_cargado = pickle.load(archivo)\n",
    "\n",
    "# ds_test = pd.read_csv('./test_normalizado_stemming.csv')\n",
    "# ds_submission = ds_test.copy()\n",
    "\n",
    "# nuevas_caracteristicas = vectorizador.transform(ds_submission.review_es)\n",
    "# predicciones_nuevas = modelo_cargado.predict(nuevas_caracteristicas)\n",
    "\n",
    "# df_submission = pd.DataFrame({'ID': ds_submission.ID, 'sentimiento': predicciones_nuevas})\n",
    "# df_submission.head()\n",
    "# df_submission.to_csv('submissions/naive_bayes/nb_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observación: este modelo overfitteó. Obtuvimos buenas métricas en entrenamiento, pero malas métricas en la submission de Kaggle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo 2: Bayes Naïve con TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a entrenar un segundo modelo Bayes Naïve, esta vez utilizando valores normalizados, computando la frecuencia inversa de cada término con TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(ds_trabajo.review_es, ds_trabajo.sentimiento, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear un objeto TfidfVectorizer con stopwords en español\n",
    "vectorizador = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "# Ajustar y transformar los datos de entrenamiento\n",
    "X_train_transformado = vectorizador.fit_transform(X_train)\n",
    "\n",
    "# Transformar los datos de prueba\n",
    "X_test_transformado = vectorizador.transform(X_test)\n",
    "\n",
    "# Crear el modelo Naive Bayes Multinomial\n",
    "modelo_2 = MultinomialNB()\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "modelo_2.fit(X_train_transformado, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando las métricas, estas dieron un poco más altas que con CountVectorizer, cercanas al 85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostrar_metricas(etiquetas_prueba, predicciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportamos el modelo en formato pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('./models/naive_bayes/nb_model2.pickle', 'wb') as archivo:\n",
    "#     pickle.dump(modelo_2, archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./models/naive_bayes/nb_model2.pickle', 'rb') as archivo:\n",
    "#     modelo_cargado = pickle.load(archivo)\n",
    "\n",
    "# ds_test = pd.read_csv('./test_normalizado_stemming.csv')\n",
    "# ds_submission = ds_test.copy()\n",
    "\n",
    "# nuevas_caracteristicas = vectorizador.transform(ds_submission.review_es)\n",
    "# predicciones_nuevas = modelo_cargado.predict(nuevas_caracteristicas)\n",
    "\n",
    "# df_submission = pd.DataFrame({'ID': ds_submission.ID, 'sentimiento': predicciones_nuevas})\n",
    "# df_submission.head()\n",
    "# df_submission.to_csv('submissions/naive_bayes/nb_submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: este modelo también overfitteó al hacer las predicciones en Kaggle (dio 85% en train y 71% en test, aproximadamente)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae0944a860fa16acdf75a2e2acdd114c1b5055b7640fecd42984d96b9abbeafd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
